# full documentation
# Domain-Aware Speech-to-Text Correction Pipeline

One repo to demonstrate three levels of domain adaptation for ASR (Quick / Medium / Long)

> Convert noisy, autogenerated captions (e.g. YouTube) into accurate transcripts for domain-heavy talks (finance, medicine, law). Example problem: the word **"moneyness"** being transcribed as *"maniness"* or *"modernism"* by a generic ASR.

---

## TL;DR

A single, progressive codebase that shows three practical solutions to improve ASR for domain-specific vocabulary:

* **Quick Fix (post-processing):** Run any ASR, then use an LLM to correct domain errors using context. Fast to prototype and low-cost.
* **Medium Fix (vocabulary biasing / glossary injection):** Use an ASR that supports custom vocabulary or phrase hints to bias outputs toward domain tokens.
* **Long Fix (fine-tuning):** Collect domain audio+transcripts and fine-tune a speech model (wav2vec2 / Whisper) for the highest long-term accuracy.

The repo contains notebooks, scripts, and a small demo web UI to compare outputs side-by-side.

---

## Goals / What you'll get

* A reproducible pipeline to ingest audio and produce three parallel outputs (raw ASR, glossary-biased ASR, LLM-corrected transcript).
* Evaluation suite with WER and domain-term metrics.
* Example dataset and prompts for finance lectures.
* A web demo (Gradio/Streamlit) to manually inspect and collect human feedback.

---

## Repo layout (recommended)

```
speech-domain-fix/
├── data/
│   ├── raw_audio/                # small sample audio (.wav/.flac)
│   ├── transcripts/              # gold transcripts for eval
│   └── glossary.txt              # domain-specific terms, one per line
├── notebooks/
│   ├── 1_quick_fix.ipynb         # ASR -> LLM post-process demo
│   ├── 2_medium_fix.ipynb        # glossary injection demo
│   └── 3_long_fix.ipynb          # fine-tuning recipe & eval
├── src/
│   ├── asr_whisper.py            # wrapper to run Whisper locally
│   ├── asr_cloud.py              # wrappers for Google/Azure speech with glossary
│   ├── llm_correction.py         # prompts & helper to call ChatGPT-style API
│   ├── finetune_asr.py           # scripts for training wav2vec2/Whisper on domain data
│   ├── eval/                     # WER, domain-term recall/precision scripts
│   └── webapp/                   # Gradio/Streamlit demo
├── models/                       # (optional) pre-trained/fine-tuned model checkpoints
├── requirements.txt
├── README.md
└── LICENSE
```

---

## Problem Explanation (detailed)

Modern ASR systems are trained on general speech. When the input domain contains unusual tokens (technical terms, names, equations), the ASR's language model often substitutes a more common-sounding word. This is especially visible in auto-generated captions on platforms like YouTube.

Consequences:

* Misleading or wrong transcripts for technical talks
* Loss of searchability (domain keywords missing)
* Bad downstream NLP (summarization, indexing, QA)

Example (finance lecture):

* Ground truth: `moneyness`
* Generic ASR: `maniness` / `modernism`

Why it happens:

* Low prior probability of domain tokens in ASR language model
* Phonetic similarity causes confusion
* Lack of domain audio during ASR training

---

## Approaches (overview + when to use)

1. **Quick Fix (LLM post-processing)** — *use when you need fast improvement, low cost, no data collection required.*
2. **Medium Fix (Glossary injection / biasing)** — *use when you can control the ASR engine and have a curated list of terms.*
3. **Long Fix (Fine-tuned ASR)** — *use when you expect to process lots of domain audio and can collect a labelled dataset. Highest accuracy but highest effort.*

---

## Implementation Details

Below are step-by-step instructions for each module. The code in `src/` and the notebooks mirror these steps.

### 1) Quick Fix — ASR + LLM Post-Processing

**Pipeline:**

1. Run a baseline ASR (Whisper, Google, or YouTube auto-captions) → `raw_transcript.txt`.
2. Pass `raw_transcript.txt` into an LLM post-processor with a prompt telling it to prefer domain vocabulary and to **preserve punctuation, equations, and timestamps** (if present).
3. Output: `llm_corrected_transcript.txt`.

**Why it works:** The LLM uses sentence-level semantics to decide that `moneyness` is a far better fit than `maniness` in a finance context.

**Prompt template (example)**

```
SYSTEM: You are a transcript-corrector specialized in finance. Keep punctuation and timestamps.
USER: Correct the transcript below. Prefer domain terms from this glossary: [moneyness, Black-Scholes, volatility].
Transcript:
"... the level of maniness ..."

Return only the corrected transcript.
```

**Implementation notes:**

* Use `openai.ChatCompletion` or similar Chat API with a temperature = 0 to make deterministic corrections.
* Keep prompts short and include a small glossary.
* Optionally add `few-shot` examples showing wrong -> correct pairs.

**Pros & Cons:**

* * Super fast and flexible.
* * No new training data required.
* * Dependent on LLM cost and latency.
* * LLM may hallucinate (rarely) — add rule-based checks for domain tokens.

### 2) Medium Fix — Glossary Injection / Phrase Hints

**Pipeline:**

* Use an ASR engine that supports biasing (Google Cloud Speech-to-Text `speechContexts`, Azure's `PhraseList`, some enterprise ASR providers).
* Provide a curated glossary file (`glossary.txt`).
* Re-run transcription with biasing enabled.

**Example (pseudo-Python for Google Cloud)**

```python
from google.cloud import speech
client = speech.SpeechClient()
config = speech.RecognitionConfig(
    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
    sample_rate_hertz=16000,
    language_code="en-US",
    speech_contexts=[speech.SpeechContext(phrases=["moneyness", "Black-Scholes"])],
)
# call recognize or long_running_recognize
```

**Implementation notes:**

* Phrase hints often accept weights/boosts; tune that value.
* This is usually much cheaper than fine-tuning and works well when you have a stable glossary.
* Some open-source ASR (vanilla Whisper) does not have built-in vocabulary biasing; look for custom forks or cloud offerings.

**Pros & Cons:**

* * Lightweight and effective for fixed vocabularies.
* * Low latency and low cost.
* * Requires an ASR that supports biasing.
* * May still fail when the word is spoken in a different accent or extremely noisy conditions.

### 3) Long Fix — Fine-Tuning an ASR Model

**Pipeline summary:**

1. Collect domain audio (lectures, podcasts) and high-quality transcripts.
2. Prepare dataset in standard format (JSONL: `{"audio_filepath":..., "text":...}` or `datasets` library format).
3. Fine-tune a speech model (wav2vec2 or Whisper) using Hugging Face `Trainer` or Fairseq training scripts.
4. Evaluate on held-out domain test set.

**Dataset tips:**

* 10–50 hours of high-quality domain data gives big gains. Even a few hours of targeted domain audio helps a lot for rare tokens.
* Include varied speakers, accents, and noise conditions.
* Align transcripts to audio (use forced alignment if necessary).

**Training notes (wav2vec2 example):**

* Use feature extractor and tokenizer appropriate for the target language.
* Suggested starting hyperparameters: lr=3e-5, batch\_size=8–32 (depending on GPU), epochs=5–30.
* Use gradient accumulation if GPU memory is limited.

**Evaluation:**

* Compute WER with `jiwer`.
* Compute domain-term recall: fraction of true domain tokens correctly transcribed.

**Pros & Cons:**

* * Best accuracy for domain tokens and overall speech.
* * Once trained, very cheap to run at inference time (no LLM costs).
* * Requires labelled data, compute resources, and ML expertise.

---

## Evaluation Plan & Metrics

* **WER (Word Error Rate)** — baseline vs. recipe outputs.
* **Domain-term recall / precision** — for glossary words.
* **Confusion matrix for domain tokens** — which tokens are confused with which.
* **Human evaluation** — small A/B test (5–10 raters) on faithfulness and readability.

Create an `eval/` script that:

* Takes two transcripts and computes WER.
* Extracts glossary words and computes recall/precision.
* Saves `results.csv` for per-file statistics.

Sample `jiwer` usage:

```python
from jiwer import wer
error = wer(reference, hypothesis)
```

---

## Demo Web App (UX)

A simple Gradio or Streamlit interface with:

* File upload (audio)
* Run options: `Raw ASR | Glossary-biased ASR | LLM-corrected` (toggle)
* Side-by-side transcripts and a diff viewer
* Button to provide feedback (accept/correct) which logs corrections for future fine-tuning (active learning)

This makes the repo interactive and great for a demo video / portfolio.

---

## Tools & Libraries (suggested)

* ASR: `openai/whisper`, `transformers` (wav2vec2), `google-cloud-speech`, `azure-cognitiveservices-speech`
* LLM: `openai` or similar Chat API
* Training: `transformers`, `datasets`, `torchaudio`, `accelerate`
* Evaluation: `jiwer`
* Web demo: `gradio` or `streamlit`
* Packaging / Deploy: `FastAPI`, `Docker`

## Sample Prompts (practical)

**Simple correction prompt**

```
You are a transcript corrector specialized in finance talks. Below is an automatically transcribed raw transcript and a glossary of domain words. Correct transcription mistakes and prefer glossary words when context fits. Preserve punctuation and inline math.

Glossary: moneyness, Black-Scholes, volatility, arbitrage

Raw:
"... measure the level of maniness using ..."

Corrected:
```

**Few-shot example**

```
Wrong: "the volatillity smile"
Correct: "the volatility smile"

Wrong: "maniness"
Correct: "moneyness"
```

Tips: set `temperature=0` and `max_tokens` to a safe limit; include `system` message instructing to only correct and not add new content.

---

## Data Format (recommended)

**JSONL** entries for Trainer / Hugging Face

```jsonl
{"audio_filepath": "data/raw_audio/lecture1.wav", "text": "This lecture covers moneyness and option pricing."}
```

**Glossary file** (glossary.txt)

```
moneyness
Black-Scholes
volatility
arbitrage
```

---

## Active Learning & Feedback Loop

* When users correct LLM outputs through the UI, store (raw\_asr, corrected\_transcript) pairs.
* Periodically curate these pairs and use them to fine-tune the LLM or as additional supervised data for ASR fine-tuning.

---

## Deployment & Cost Considerations

* Quick Fix cost: LLM inference cost per transcript (important at scale).
* Medium Fix cost: ASR provider cost per minute; usually lower than LLM.
* Long Fix cost: one-time compute for training; low inference cost thereafter.

Deployment options:

* Provide a REST API (FastAPI) that runs the ASR + choice of fixes.
* For heavy workloads, separate the correction step (LLM) into an async job queue.

---

## Ethics, Privacy & Licensing

* Always obtain consent/rights for audio you collect (lectures vs. public YouTube content).
* Redact sensitive personal information in transcripts.
* Document dataset licenses in the repo.

---

## Future Work / Extensions

* **Streaming real-time correction**: low-latency LLMs or on-device solutions.
* **Multilingual support** and cross-language glossary mapping.
* **On-device models**: quantize (TFLite / ONNX) for mobile apps.
* **Model distillation**: distill fine-tuned models to smaller architectures for faster inference.
* **Speaker diarization** + attribution of domain terms to speakers.
* **LaTeX / equation-aware correction** for talks with inline math.
* **Confidence scoring** for each corrected token (combine ASR token score + LLM confidence heuristic).
* **Auto-glossary extraction**: mine domain terms automatically from corpora and rank by importance.

---

## How to Start (Quick dev checklist)

1. `git clone` this repo
2. create virtualenv & `pip install -r requirements.txt`
3. drop a small test `.wav` into `data/raw_audio/`
4. run `python src/asr_whisper.py --input data/raw_audio/test.wav --output tmp/raw.txt`
5. run `python src/llm_correction.py --input tmp/raw.txt --glossary data/glossary.txt --output tmp/corrected.txt`
6. open `notebooks/1_quick_fix.ipynb` to experiment with prompts and evaluation

---

## Contributing

* Add more domain datasets (medicine, law).
* Add additional ASR providers for glossary injection.
* Improve evaluation: add more human-rated testsets.

---

## License

Pick an open-source license (MIT / Apache 2.0). Make sure any datasets you include are permitted for redistribution.

---

## Contact

If you want, I can now:

* generate the `llm_correction.py` module with prompt code and example calls, **or**
* generate the Gradio demo app code that runs all three approaches side-by-side.

Tell me which file you'd like first and I'll drop a ready-to-run script/notebook in the repo.
